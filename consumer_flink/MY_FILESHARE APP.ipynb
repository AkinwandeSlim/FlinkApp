{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c05522a2",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyflink'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Tuple\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyflink\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtable\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataTypes, Row\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyflink\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtable\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RowType\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyflink\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtable\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mudf\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m udf\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyflink'"
     ]
    }
   ],
   "source": [
    "from typing import Tuple\n",
    "from pyflink.table import DataTypes, Row\n",
    "from pyflink.table.types import RowType\n",
    "from pyflink.table.udf import udf\n",
    "import os\n",
    "from pyflink.datastream import StreamExecutionEnvironment\n",
    "from pyflink.table import StreamTableEnvironment, EnvironmentSettings, DataTypes\n",
    "from pyflink.table.window import Slide\n",
    "from pyflink.table.window import Tumble\n",
    "from pyflink.table.udf import ScalarFunction , TableFunction, udf\n",
    "from pyflink.table import DataTypes\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import json\n",
    "import requests\n",
    "from pyflink.table.types import RowType, DataTypes\n",
    "from beta_distribution_drift_detector.bdddc import BDDDC\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "env = StreamExecutionEnvironment.get_execution_environment()\n",
    "env.set_parallelism(1)\n",
    "env_settings = (\n",
    "    EnvironmentSettings.new_instance().in_streaming_mode().use_blink_planner().build()\n",
    ")\n",
    "st_env = StreamTableEnvironment.create(env, environment_settings=env_settings)\n",
    "st_env.get_config().get_configuration().set_boolean(\"python.fn-execution.memory.managed\", True)\n",
    "dir_kafka_sql_connect = os.path.join(os.path.abspath(os.path.dirname(__file__)),\n",
    "                                     'flink-sql-connector-kafka_2.11-1.11.2.jar')\n",
    "st_env.get_config().get_configuration().set_string(\"pipeline.jars\", 'file://' + dir_kafka_sql_connect)\n",
    "dir_requirements  =  '/requirements. txt'\n",
    "dir_cache  = '/cached_dir/'\n",
    "if  os.path.exists ( dir_requirements ) :\n",
    "    if  os . path . exists ( dir_cache ):\n",
    "        st_env.set_python_requirements ( dir_requirements , dir_cache ) \n",
    "    else :\n",
    "        st_env.set_python_requirements ( dir_requirements)\n",
    "        \n",
    "        \n",
    "        \n",
    "class AccuracyPrecisionTuple:\n",
    "    def __init__(self, accuracy, precision):\n",
    "        self.accuracy = accuracy\n",
    "        self.precision = precision\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "class Model(ScalarFunction):\n",
    "    def __init__(self):\n",
    "        \n",
    "        # load the model\n",
    "        self.model_name = 'online_ml_model'\n",
    "        self.redis_params = dict(host='localhost', password='redis_password', port=6379, db=0)\n",
    "        self.clf = self.load_model()\n",
    "\n",
    "        self.interval_dump_seconds = 30  # Model save interval is 30 seconds\n",
    "        self.last_dump_time = datetime.now() # last time the model was saved\n",
    "\n",
    "        # domain of y\n",
    "        self.classes = list(range(2))\n",
    "        self.metric_counter = None # The number of all samples from the beginning of the job to the present\n",
    "        self.metric_predict_acc = 0 # model prediction accuracy (evaluated with the past 10 samples)\n",
    "        self.metric_distribution_y = None # distribution of label y\n",
    "        self.metric_total_60_sec = None # number of samples trained in the past 10 seconds\n",
    "        self.metric_right_60_sec = None # The number of correctly predicted samples in the past 10 seconds\n",
    "        \n",
    "        self.drift_check_interval =timedelta(seconds=60)\n",
    "        self.drift_detector=BDDDC()\n",
    "         # ... existing code ...\n",
    "        self.accuracy = 0.0\n",
    "        self.precision = 0.0   \n",
    "        \n",
    "\n",
    "    def open(self, function_context):\n",
    "        \"\"\"\n",
    "        Access the indicator system and register the indicators so that you can view the operation of the algorithm in real time on the webui (localhost:8081).\n",
    "        :param function_context:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        metric_group = function_context.get_metric_group().add_group(\"online_ml\")\n",
    "\n",
    "        self.metric_counter = metric_group.counter('sample_count') # number of trained samples\n",
    "        metric_group.gauge(\"prediction_acc\", lambda: int(self.metric_predict_acc * 100))\n",
    "        self.metric_distribution_y = metric_group.distribution(\"metric_distribution_y\")\n",
    "        self.metric_total_60_sec = metric_group.meter(\"total_60_sec\", time_span_in_seconds=60)\n",
    "        self.metric_right_60_sec = metric_group.meter(\"right_60_sec\", time_span_in_seconds=60)\n",
    "        \n",
    " \n",
    "\n",
    "        \n",
    "       \n",
    "    def eval(self, attrib1, attrib2, attrib3, label):\n",
    "        \"\"\"\n",
    "         Model training\n",
    "        :param attrib1: attribute 1 of the input data, float\n",
    "        :param attrib2: attribute 2 of the input data, float\n",
    "        :param attrib3: attribute 3 of the input data, float\n",
    "        :param label: label of the input data, int\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        # Online learning, that is, training models one by one\n",
    "        x = [attrib1, attrib2, attrib3]\n",
    "        y = [label]\n",
    "        self.clf.partial_fit([x], [y], classes=self.classes)\n",
    "\n",
    "        # Predict the current\n",
    "        y_pred = self.clf.predict([x])[0]\n",
    "\n",
    "        # Update drift detector\n",
    "        self.drift_detector.add_element(np.array(y_pred, ndmin = 1), np.array(label, ndmin = 1))\n",
    "\n",
    "        # Check for concept drift\n",
    "        if self.drift_detector.detected_change():\n",
    "            print(\"Concept drift detected at time \", time.time())\n",
    "            self.clf.partial_fit([x], [y], classes=self.classes)\n",
    "            self.drift_detector=BDDDC()\n",
    "\n",
    "        # update metrics\n",
    "        self.metric_counter.inc(1) # Number of trained samples + 1\n",
    "        self.metric_total_60_sec.mark_event(1) # Update the Meter: + 1 for a piece of data, and count the sample size within 10 seconds\n",
    "\n",
    "        # calculate TP, TN, FP, FN values\n",
    "        if y_pred == label:\n",
    "            self.metric_right_60_sec.mark_event(1) # Update the Meter: + 1 for a piece of data, and count the sample size within 10 seconds\n",
    "            self.metric_predict_acc = self.metric_right_60_sec.get_count() / self.metric_total_60_sec.get_count() # Accuracy _ _\n",
    "            self.metric_distribution_y.update(label) # update distribution Distribution: the number of trained samples + 1\n",
    "            if y_pred == 1:\n",
    "                TP = 1\n",
    "                TN = 0\n",
    "                FP = 0\n",
    "                FN = 0\n",
    "            else:\n",
    "                TP = 0\n",
    "                TN = 1\n",
    "                FP = 0\n",
    "                FN = 0\n",
    "        else:\n",
    "            self.metric_distribution_y.update(label) # update distribution Distribution: the number of trained samples + 1\n",
    "            if y_pred == 1:\n",
    "                TP = 0\n",
    "                TN = 0\n",
    "                FP = 1\n",
    "                FN = 0\n",
    "            else:\n",
    "                TP = 0\n",
    "                TN = 0\n",
    "                FP = 0\n",
    "                FN = 1\n",
    "\n",
    "        # calculate precision, recall, F1-score, and accuracy\n",
    "        precision = TP / (TP + FP) if TP + FP != 0 else 0\n",
    "        recall = TP / (TP + FN) if TP + FN != 0 else 0\n",
    "        f1_score = 2 * precision * recall / (precision + recall) if precision + recall != 0 else 0\n",
    "        accuracy = self.metric_predict_acc\n",
    "\n",
    "         # Calculate accuracy\n",
    "        accuracy = self.metric_right_60_sec.get_count() / self.metric_total_60_sec.get_count()\n",
    "     \n",
    "        # Update self.accuracy and self.precision\n",
    "        self.accuracy = accuracy  # Replace with your actual computation\n",
    "        self.precision =precision  # Replace with your actual computation\n",
    "\n",
    "        # Return the values as a tuple\n",
    "        return AccuracyPrecisionTuple(accuracy,precision)\n",
    "        \n",
    "        \n",
    "    def load_model(self):\n",
    "        \"\"\"\n",
    "        Load the model, if there is a model in redis, it will be loaded from redis first,\n",
    "        otherwise a new model will be initialized\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        import redis\n",
    "        import pickle\n",
    "        import logging\n",
    "        from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "        r = redis.StrictRedis(**self.redis_params)\n",
    "        clf = None\n",
    "\n",
    "        try:\n",
    "            clf = pickle.loads(r.get(self.model_name))\n",
    "        except TypeError:\n",
    "            logging.info('There is no model with the specified name in Redis, so a new model is initialized')\n",
    "        except (redis.exceptions.RedisError, TypeError, Exception):\n",
    "            logging.warning('Redis encountered an exception, so initialize a new model')\n",
    "        finally:\n",
    "            clf = clf or SGDClassifier(alpha=0.01, loss='log', penalty='l1')\n",
    "\n",
    "    return clf\n",
    "\n",
    "    def  dump_model ( self ):\n",
    "     \"\"\"\n",
    "    Saves the model when the specified time interval has elapsed since the last attempt to save the model\n",
    "    :return:\n",
    "    \"\"\"\n",
    "     import  pickle\n",
    "     import  redis\n",
    "     import  logging\n",
    "\n",
    "     if ( datetime . now () -  self . last_dump_time ). seconds  >=  self . interval_dump_seconds :\n",
    "         r  =  redis . StrictRedis ( ** self . redis_params )\n",
    "\n",
    "         try :\n",
    "             r . set ( self . model_name , pickle . dumps ( self . clf , protocol = pickle . HIGHEST_PROTOCOL ))\n",
    "         except ( redis . exceptions . RedisError , TypeError , Exception ):\n",
    "             logging.warning ( ' Unable to connect to Redis to store model data' )\n",
    "\n",
    "         self . last_dump_time  =  datetime . now () # Whether the update is successful or not, update the save time\n",
    "  \n",
    "\n",
    "\n",
    "\n",
    "# DataTypes.FIELD(\"accuracy\", DataTypes.FLOAT())\n",
    "# Define the result type for AccuracyPrecisionTuple\n",
    "result_type = RowType([DataTypes.FIELD(\"accuracy\", DataTypes.FLOAT()),\n",
    "                      DataTypes.FIELD(\"precision\", DataTypes.FLOAT())])\n",
    "\n",
    "\n",
    "# model = udf(Model(), input_types=[DataTypes.FLOAT(), DataTypes.FLOAT(), DataTypes.FLOAT(), DataTypes.INT()],\n",
    "#             result_type=DataTypes.FLOAT())\n",
    "\n",
    "model = udf(Model(), input_types=[DataTypes.FLOAT(), DataTypes.FLOAT(), DataTypes.FLOAT(), DataTypes.INT()],\n",
    "            result_type=result_type)\n",
    "st_env.register_function('train_predict', model)\n",
    "\n",
    "# model_udtf = udf(ModelUDTF(), input_types=[DataTypes.FLOAT(), DataTypes.FLOAT(), DataTypes.FLOAT(), DataTypes.INT()],\n",
    "#                  result_type=DataTypes.ROW([DataTypes.FIELD(\"accuracy\", DataTypes.FLOAT()), DataTypes.FIELD(\"precision\", DataTypes.FLOAT())]))\n",
    "# st_env.register_function('result_predict', model_udtf)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Define source\n",
    "st_env.execute_sql(\n",
    "    f\"\"\"\n",
    "    CREATE TABLE source (\n",
    "        attrib1 DOUBLE,\n",
    "        attrib2 DOUBLE,\n",
    "        attrib3 DOUBLE,\n",
    "        label INT,\n",
    "        ts BIGINT,\n",
    "        rowtime as TO_TIMESTAMP(FROM_UNIXTIME(ts, 'yyyy-MM-dd HH:mm:ss')),\n",
    "        WATERMARK FOR rowtime AS rowtime - INTERVAL '5' SECOND\n",
    "    ) WITH (\n",
    "        'connector' ='kafka-0.11',\n",
    "        'topic' = '{os.environ[\"KAFKA_TOPIC\"]}',\n",
    "        'scan.startup.mode' = 'latest-offset',\n",
    "        'properties.bootstrap.servers' = '{os.environ[\"KAFKA_HOST\"]}',\n",
    "        'properties.zookeeper.connect' = '{os.environ[\"ZOOKEEPER_HOST\"]}',\n",
    "        'properties.group.id' = '{os.environ[\"KAFKA_CONSUMER_GROUP\"]}',\n",
    "        'format' = 'json'\n",
    "    )\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "# # Define output sink\n",
    "# st_env.execute_sql(\n",
    "#     \"\"\"\n",
    "#     CREATE TABLE sink (\n",
    "#         accuracy FLOAT,\n",
    "#         precision FLOAT,\n",
    "#         window_start TIMESTAMP(3),\n",
    "#         window_end TIMESTAMP(3)\n",
    "#      ) WITH (\n",
    "#          'connector' = 'print',\n",
    "#          'print-identifier' = 'Drift data: '\n",
    "#      )\n",
    "#     \"\"\"\n",
    "# )\n",
    "\n",
    "# # Calculate average accuracy per window and insert into sink\n",
    "# st_env.from_path(\"source\") \\\n",
    "#     .window(Tumble.over(\"60.seconds\").on(\"rowtime\").alias(\"w\")) \\\n",
    "#     .group_by(\"w\") \\\n",
    "#     .select(\"AVG(train_predict(attrib1, attrib2, attrib3, label)) AS accuracy, w.start AS window_start, w.end AS window_end\") \\\n",
    "#     .insert_into(\"sink\")\n",
    "\n",
    "\n",
    "# Define output sink\n",
    "st_env.execute_sql(\n",
    "    \"\"\"\n",
    "    CREATE TABLE sink (\n",
    "        `accuracy` FLOAT,\n",
    "        `precision` FLOAT,\n",
    "        window_start TIMESTAMP(3),\n",
    "        window_end TIMESTAMP(3)\n",
    "     ) WITH (\n",
    "         'connector' = 'print',\n",
    "         'print-identifier' = 'Drift data: '\n",
    "     )\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# Calculate average accuracy and precision per window and insert into sink\n",
    "st_env.from_path(\"source\") \\\n",
    "    .window(Tumble.over(\"60.seconds\").on(\"rowtime\").alias(\"w\")) \\\n",
    "    .group_by(\"w\") \\\n",
    "    .select(\"AVG(train_predict(attrib1, attrib2, attrib3, label).f0) AS accuracy, \"\n",
    "            \"AVG(train_predict(attrib1, attrib2, attrib3, label).f1) AS precision, \"\n",
    "            \"w.start AS window_start, w.end AS window_end\") \\\n",
    "    .insert_into(\"sink\")\n",
    "\n",
    "# Execute the job\n",
    "st_env.execute(\"PyFlink job\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
